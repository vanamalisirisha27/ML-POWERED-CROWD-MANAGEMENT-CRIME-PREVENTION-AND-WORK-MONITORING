{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\prasa\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch) (2021.10.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: networkx in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Pillow in c:\\users\\prasa\\anaconda3\\lib\\site-packages (8.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\prasa\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision) (2.6.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision) (2021.10.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision) (1.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision) (3.3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.2->torchvision) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\prasa\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.2->torchvision) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20644/1939872320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mtrain_loss_custom\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[0mtrain_loss_custom\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriminal_train_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0mtrain_accuracy_custom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_custom\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;31m# Validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data import RandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Define paths to the criminal face dataset and the general face dataset\n",
    "criminal_train_path = 'C:/Users/prasa/Desktop/4-2/face/face/aligned_faces/train'\n",
    "criminal_val_path = 'C:/Users/prasa/Desktop/4-2/face/face/aligned_faces/val'\n",
    "criminal_test_path = 'C:/Users/prasa/Desktop/4-2/face/face/aligned_faces/test'\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # Add random rotation\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Add color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load criminal face datasets with data augmentation\n",
    "criminal_train_dataset = ImageFolder(root=criminal_train_path, transform=transform)\n",
    "\n",
    "# Calculate class weights for handling class imbalance\n",
    "class_weights = torch.zeros(len(criminal_train_dataset.classes))\n",
    "for _, label in criminal_train_dataset:\n",
    "    class_weights[label] += 1\n",
    "class_weights = 1 / class_weights\n",
    "sample_weights = [class_weights[label] for _, label in criminal_train_dataset]\n",
    "\n",
    "# Use weighted random sampler for handling class imbalance\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "# Define data loaders for training, validation, and testing sets\n",
    "batch_size = 32\n",
    "criminal_train_loader = DataLoader(criminal_train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "criminal_val_dataset = ImageFolder(root=criminal_val_path, transform=transform)\n",
    "criminal_val_loader = DataLoader(criminal_val_dataset, batch_size=batch_size)\n",
    "criminal_test_dataset = ImageFolder(root=criminal_test_path, transform=transform)\n",
    "criminal_test_loader = DataLoader(criminal_test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "class FaceSwinIR(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FaceSwinIR, self).__init__()\n",
    "        # Define the layers of the FaceSwinIR model\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(512 * 16 * 16, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = x.view(-1, 512 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Initialize the FaceSwinIR model with the number of classes in the custom dataset\n",
    "num_classes_custom = 10  # Update with the actual number of classes in the custom dataset\n",
    "model_custom = FaceSwinIR(num_classes_custom)\n",
    "\n",
    "# Define loss function and optimizer for custom dataset\n",
    "criterion_custom = nn.CrossEntropyLoss()\n",
    "optimizer_custom = optim.Adam(model_custom.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for custom dataset\n",
    "num_epochs_custom = 30\n",
    "best_val_loss_custom = float('inf')\n",
    "for epoch in range(num_epochs_custom):\n",
    "    # Training\n",
    "    model_custom.train()\n",
    "    train_loss_custom = 0.0\n",
    "    correct_custom = 0\n",
    "    total_custom = 0\n",
    "    for images, labels in criminal_train_loader:\n",
    "        optimizer_custom.zero_grad()\n",
    "        outputs = model_custom(images)\n",
    "        loss = criterion_custom(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_custom.step()\n",
    "        train_loss_custom += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_custom += labels.size(0)\n",
    "        correct_custom += (predicted == labels).sum().item()\n",
    "    train_loss_custom /= len(criminal_train_loader.dataset)\n",
    "    train_accuracy_custom = correct_custom / total_custom\n",
    "\n",
    "    # Validation\n",
    "    model_custom.eval()\n",
    "    val_loss_custom = 0.0\n",
    "    correct_custom = 0\n",
    "    total_custom = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in criminal_val_loader:\n",
    "            outputs = model_custom(images)\n",
    "            loss = criterion_custom(outputs, labels)\n",
    "            val_loss_custom += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_custom += labels.size(0)\n",
    "            correct_custom += (predicted == labels).sum().item()\n",
    "        val_loss_custom /= len(criminal_val_loader.dataset)\n",
    "        val_accuracy_custom = correct_custom / total_custom\n",
    "\n",
    "    # Print training progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_custom}], '\n",
    "          f'Train Loss: {train_loss_custom:.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy_custom:.2%}, '\n",
    "          f'Val Loss: {val_loss_custom:.4f}, '\n",
    "          f'Val Accuracy: {val_accuracy_custom:.2%}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model_custom.state_dict(), 'criminal_face_detection_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss on Custom Dataset: 2.3002\n",
      "Test Accuracy on Custom Dataset: 22.86%\n"
     ]
    }
   ],
   "source": [
    "# Define testing loop for custom dataset\n",
    "model_custom.eval()\n",
    "test_loss_custom = 0.0\n",
    "correct_custom = 0\n",
    "total_custom = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in criminal_test_loader:\n",
    "        outputs = model_custom(images)\n",
    "        loss = criterion_custom(outputs, labels)\n",
    "        test_loss_custom += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_custom += labels.size(0)\n",
    "        correct_custom += (predicted == labels).sum().item()\n",
    "test_loss_custom /= len(criminal_test_loader.dataset)\n",
    "test_accuracy_custom = correct_custom / total_custom\n",
    "\n",
    "print(f'Test Loss on Custom Dataset: {test_loss_custom:.4f}')\n",
    "print(f'Test Accuracy on Custom Dataset: {test_accuracy_custom:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_vggface in c:\\users\\vanam\\anaconda3\\lib\\site-packages (0.6)\n",
      "Requirement already satisfied: h5py in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_vggface) (3.6.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_vggface) (1.12.0)\n",
      "Requirement already satisfied: keras in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_vggface) (2.2.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_vggface) (6.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_vggface) (1.22.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_vggface) (9.0.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_vggface) (1.16.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras->keras_vggface) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras->keras_vggface) (1.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_applications in c:\\users\\vanam\\anaconda3\\lib\\site-packages (1.0.8)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_applications) (1.22.4)\n",
      "Requirement already satisfied: h5py in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras_applications) (3.6.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.4 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.22.4)\n",
      "Requirement already satisfied: h5py in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras==2.2.4) (3.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.1.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras==2.2.4) (6.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\vanam\\anaconda3\\lib\\site-packages (from keras==2.2.4) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.15.5 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1)\n",
      "ERROR: No matching distribution found for tensorflow==1.15.5\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\vanam\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install keras_vggface\n",
    "%pip install keras_applications\n",
    "%pip install keras==2.2.4\n",
    "%pip install tensorflow==1.15.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\vanam\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3110: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if training is 1 or training is True:\n",
      "C:\\Users\\vanam\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3116: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif training is 0 or training is False:\n",
      "C:\\Users\\vanam\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3110: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if training is 1 or training is True:\n",
      "C:\\Users\\vanam\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3116: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif training is 0 or training is False:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Initialize the VGGFaceRecognizer model with the number of classes in the custom dataset\u001b[39;00m\n\u001b[0;32m     54\u001b[0m num_classes_custom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(criminal_train_dataset\u001b[38;5;241m.\u001b[39mclasses)\n\u001b[1;32m---> 55\u001b[0m model_custom \u001b[38;5;241m=\u001b[39m \u001b[43mVGGFaceRecognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes_custom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Define loss function and optimizer for custom dataset\u001b[39;00m\n\u001b[0;32m     58\u001b[0m criterion_custom \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mVGGFaceRecognizer.__init__\u001b[1;34m(self, num_classes)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28msuper\u001b[39m(VGGFaceRecognizer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load pre-trained VGGFace model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvggface \u001b[38;5;241m=\u001b[39m \u001b[43mVGGFace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Freeze the VGGFace layers\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvggface\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_vggface\\vggface.py:94\u001b[0m, in \u001b[0;36mVGGFace\u001b[1;34m(include_top, model, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvggface\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m include_top \u001b[38;5;129;01mand\u001b[39;00m classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8631\u001b[39m:\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     91\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf using `weights` as vggface original with `include_top`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as true, `classes` should be 8631\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRESNET50\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msenet50\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_vggface\\models.py:219\u001b[0m, in \u001b[0;36mRESNET50\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m    211\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m _obtain_input_shape(input_shape,\n\u001b[0;32m    212\u001b[0m                                   default_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m,\n\u001b[0;32m    213\u001b[0m                                   min_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m    214\u001b[0m                                   data_format\u001b[38;5;241m=\u001b[39mK\u001b[38;5;241m.\u001b[39mimage_data_format(),\n\u001b[0;32m    215\u001b[0m                                   require_flatten\u001b[38;5;241m=\u001b[39minclude_top,\n\u001b[0;32m    216\u001b[0m                                   weights\u001b[38;5;241m=\u001b[39mweights)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     img_input \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m K\u001b[38;5;241m.\u001b[39mis_keras_tensor(input_tensor):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py:175\u001b[0m, in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m    174\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m--> 175\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m \u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m                         \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Return tensor including _keras_shape and _keras_history.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Note that in this case train_output and test_output are the same pointer.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m input_layer\u001b[38;5;241m.\u001b[39m_inbound_nodes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput_tensors\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py:91\u001b[0m, in \u001b[0;36mgenerate_legacy_interface.<locals>.legacy_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     signature \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     89\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate your `\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m object_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` call to the \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     90\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeras 2 API: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m signature, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py:39\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[0;32m     38\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 39\u001b[0m     name \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_uid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28msuper\u001b[39m(InputLayer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74\u001b[0m, in \u001b[0;36mget_uid\u001b[1;34m(prefix)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the uid for the default graph.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m# Arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    A unique identifier for the graph.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _GRAPH_UID_DICTS\n\u001b[1;32m---> 74\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_graph\u001b[49m()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _GRAPH_UID_DICTS:\n\u001b[0;32m     76\u001b[0m     _GRAPH_UID_DICTS[graph] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class VGGFaceRecognizer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGGFaceRecognizer, self).__init__()\n",
    "        # Load pre-trained VGGFace model\n",
    "        self.vggface = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "        # Freeze the VGGFace layers\n",
    "        for param in self.vggface.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Add fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = preprocess_input(x)  # Preprocess input for VGGFace\n",
    "        x = self.vggface(x)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define paths to the criminal face dataset\n",
    "criminal_train_path = 'C:/crowdcrime/yolov8-streamlit-detection-tracking/face/aligned_faces/train'\n",
    "criminal_val_path = 'C:/crowdcrime/yolov8-streamlit-detection-tracking/face/aligned_faces/val'\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize for VGGFace\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for VGGFace\n",
    "])\n",
    "\n",
    "# Load criminal face datasets with data augmentation\n",
    "criminal_train_dataset = ImageFolder(root=criminal_train_path, transform=transform)\n",
    "criminal_val_dataset = ImageFolder(root=criminal_val_path, transform=transform)\n",
    "\n",
    "# Define data loaders for training and validation sets\n",
    "batch_size = 32\n",
    "criminal_train_loader = DataLoader(criminal_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "criminal_val_loader = DataLoader(criminal_val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the VGGFaceRecognizer model with the number of classes in the custom dataset\n",
    "num_classes_custom = len(criminal_train_dataset.classes)\n",
    "model_custom = VGGFaceRecognizer(num_classes_custom)\n",
    "\n",
    "# Define loss function and optimizer for custom dataset\n",
    "criterion_custom = nn.CrossEntropyLoss()\n",
    "optimizer_custom = optim.Adam(model_custom.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for custom dataset\n",
    "num_epochs_custom = 30\n",
    "for epoch in range(num_epochs_custom):\n",
    "    # Training\n",
    "    model_custom.train()\n",
    "    train_loss_custom = 0.0\n",
    "    correct_custom = 0\n",
    "    total_custom = 0\n",
    "    for images, labels in criminal_train_loader:\n",
    "        optimizer_custom.zero_grad()\n",
    "        outputs = model_custom(images)\n",
    "        loss = criterion_custom(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_custom.step()\n",
    "        train_loss_custom += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_custom += labels.size(0)\n",
    "        correct_custom += (predicted == labels).sum().item()\n",
    "    train_loss_custom /= len(criminal_train_loader.dataset)\n",
    "    train_accuracy_custom = correct_custom / total_custom\n",
    "\n",
    "    # Validation\n",
    "    model_custom.eval()\n",
    "    val_loss_custom = 0.0\n",
    "    correct_custom = 0\n",
    "    total_custom = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in criminal_val_loader:\n",
    "            outputs = model_custom(images)\n",
    "            loss = criterion_custom(outputs, labels)\n",
    "            val_loss_custom += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_custom += labels.size(0)\n",
    "            correct_custom += (predicted == labels).sum().item()\n",
    "        val_loss_custom /= len(criminal_val_loader.dataset)\n",
    "        val_accuracy_custom = correct_custom / total_custom\n",
    "\n",
    "    # Print training progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_custom}], '\n",
    "          f'Train Loss: {train_loss_custom:.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy_custom:.2%}, '\n",
    "          f'Val Loss: {val_loss_custom:.4f}, '\n",
    "          f'Val Accuracy: {val_accuracy_custom:.2%}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model_custom.state_dict(), 'criminal_face_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 66>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Initialize the VGGFaceRecognizer model with the number of classes in the custom dataset\u001b[39;00m\n\u001b[0;32m     65\u001b[0m num_classes_custom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(criminal_train_dataset\u001b[38;5;241m.\u001b[39mclasses)\n\u001b[1;32m---> 66\u001b[0m model_custom \u001b[38;5;241m=\u001b[39m \u001b[43mVGGFaceRecognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes_custom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Define loss function and optimizer for custom dataset\u001b[39;00m\n\u001b[0;32m     69\u001b[0m criterion_custom \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mVGGFaceRecognizer.__init__\u001b[1;34m(self, num_classes)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28msuper\u001b[39m(VGGFaceRecognizer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Load pre-trained VGGFace model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvggface \u001b[38;5;241m=\u001b[39m \u001b[43mVGGFace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Freeze the VGGFace layers\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvggface\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_vggface\\vggface.py:94\u001b[0m, in \u001b[0;36mVGGFace\u001b[1;34m(include_top, model, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvggface\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m include_top \u001b[38;5;129;01mand\u001b[39;00m classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8631\u001b[39m:\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     91\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf using `weights` as vggface original with `include_top`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as true, `classes` should be 8631\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRESNET50\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msenet50\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_vggface\\models.py:219\u001b[0m, in \u001b[0;36mRESNET50\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m    211\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m _obtain_input_shape(input_shape,\n\u001b[0;32m    212\u001b[0m                                   default_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m,\n\u001b[0;32m    213\u001b[0m                                   min_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m    214\u001b[0m                                   data_format\u001b[38;5;241m=\u001b[39mK\u001b[38;5;241m.\u001b[39mimage_data_format(),\n\u001b[0;32m    215\u001b[0m                                   require_flatten\u001b[38;5;241m=\u001b[39minclude_top,\n\u001b[0;32m    216\u001b[0m                                   weights\u001b[38;5;241m=\u001b[39mweights)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     img_input \u001b[38;5;241m=\u001b[39m \u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m K\u001b[38;5;241m.\u001b[39mis_keras_tensor(input_tensor):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py:175\u001b[0m, in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m    174\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m--> 175\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m \u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m                         \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Return tensor including _keras_shape and _keras_history.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Note that in this case train_output and test_output are the same pointer.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m input_layer\u001b[38;5;241m.\u001b[39m_inbound_nodes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput_tensors\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py:91\u001b[0m, in \u001b[0;36mgenerate_legacy_interface.<locals>.legacy_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     signature \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     89\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate your `\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m object_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` call to the \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     90\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeras 2 API: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m signature, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py:39\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[0;32m     38\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 39\u001b[0m     name \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_uid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28msuper\u001b[39m(InputLayer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74\u001b[0m, in \u001b[0;36mget_uid\u001b[1;34m(prefix)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the uid for the default graph.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m# Arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    A unique identifier for the graph.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _GRAPH_UID_DICTS\n\u001b[1;32m---> 74\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_graph\u001b[49m()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _GRAPH_UID_DICTS:\n\u001b[0;32m     76\u001b[0m     _GRAPH_UID_DICTS[graph] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "# Define paths to the criminal face dataset and the general face dataset\n",
    "criminal_train_path = 'C:/crowdcrime/yolov8-streamlit-detection-tracking/face/aligned_faces/train'\n",
    "criminal_val_path = 'C:/crowdcrime/yolov8-streamlit-detection-tracking/face/aligned_faces/val'\n",
    "criminal_test_path = 'C:/crowdcrime/yolov8-streamlit-detection-tracking/face/aligned_faces/test'\n",
    "\n",
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resizing for VGGFace input\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),  # Add random rotation\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Add color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load criminal face datasets with data augmentation\n",
    "criminal_train_dataset = ImageFolder(root=criminal_train_path, transform=transform)\n",
    "\n",
    "# Calculate class weights for handling class imbalance\n",
    "class_weights = torch.zeros(len(criminal_train_dataset.classes))\n",
    "for _, label in criminal_train_dataset:\n",
    "    class_weights[label] += 1\n",
    "class_weights = 1 / class_weights\n",
    "sample_weights = [class_weights[label] for _, label in criminal_train_dataset]\n",
    "\n",
    "# Use weighted random sampler for handling class imbalance\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "# Define data loaders for training, validation, and testing sets\n",
    "batch_size = 32\n",
    "criminal_train_loader = DataLoader(criminal_train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "criminal_val_dataset = ImageFolder(root=criminal_val_path, transform=transform)\n",
    "criminal_val_loader = DataLoader(criminal_val_dataset, batch_size=batch_size)\n",
    "criminal_test_dataset = ImageFolder(root=criminal_test_path, transform=transform)\n",
    "criminal_test_loader = DataLoader(criminal_test_dataset, batch_size=batch_size)\n",
    "\n",
    "class VGGFaceRecognizer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGGFaceRecognizer, self).__init__()\n",
    "        # Load pre-trained VGGFace model\n",
    "        self.vggface = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "        # Freeze the VGGFace layers\n",
    "        for param in self.vggface.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Define additional layers for classification\n",
    "        self.fc = nn.Linear(2048, num_classes)  # ResNet50 outputs 2048 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through VGGFace model\n",
    "        x = self.vggface(x)\n",
    "        # Perform classification\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the VGGFaceRecognizer model with the number of classes in the custom dataset\n",
    "num_classes_custom = len(criminal_train_dataset.classes)\n",
    "model_custom = VGGFaceRecognizer(num_classes_custom)\n",
    "\n",
    "# Define loss function and optimizer for custom dataset\n",
    "criterion_custom = nn.CrossEntropyLoss()\n",
    "optimizer_custom = optim.Adam(model_custom.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for custom dataset\n",
    "num_epochs_custom = 30\n",
    "best_val_loss_custom = float('inf')\n",
    "for epoch in range(num_epochs_custom):\n",
    "    # Training\n",
    "    model_custom.train()\n",
    "    train_loss_custom = 0.0\n",
    "    correct_custom = 0\n",
    "    total_custom = 0\n",
    "    for images, labels in criminal_train_loader:\n",
    "        optimizer_custom.zero_grad()\n",
    "        outputs = model_custom(images)\n",
    "        loss = criterion_custom(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_custom.step()\n",
    "        train_loss_custom += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_custom += labels.size(0)\n",
    "        correct_custom += (predicted == labels).sum().item()\n",
    "    train_loss_custom /= len(criminal_train_loader.dataset)\n",
    "    train_accuracy_custom = correct_custom / total_custom\n",
    "\n",
    "    # Validation\n",
    "    model_custom.eval()\n",
    "    val_loss_custom = 0.0\n",
    "    correct_custom = 0\n",
    "    total_custom = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in criminal_val_loader:\n",
    "            outputs = model_custom(images)\n",
    "            loss = criterion_custom(outputs, labels)\n",
    "            val_loss_custom += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_custom += labels.size(0)\n",
    "            correct_custom += (predicted == labels).sum().item()\n",
    "        val_loss_custom /= len(criminal_val_loader.dataset)\n",
    "        val_accuracy_custom = correct_custom / total_custom\n",
    "\n",
    "    # Print training progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_custom}], '\n",
    "          f'Train Loss: {train_loss_custom:.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy_custom:.2%}, '\n",
    "          f'Val Loss: {val_loss_custom:.4f}, '\n",
    "          f'Val Accuracy: {val_accuracy_custom:.2%}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model_custom.state_dict(), 'criminal_face_detection_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
